{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendations\n",
    "\n",
    "The goal of the project is to build a recommender system that suggests relevant books based on the person's interests.\n",
    "\n",
    "## Collaborative filtering using `surprise` library: \n",
    "\n",
    "## Model training and evaluation\n",
    "\n",
    "Evaluate a few models that performed best during cross-validation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible experiments\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithms from Surprise\n",
    "\n",
    "from surprise import NormalPredictor, SVDpp, SlopeOne, KNNBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load  and prepare the data\n",
    "Create the Surprise Datasets for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../data/ratings_train.csv'\n",
    "path_test = '../data/ratings_test.csv'\n",
    "\n",
    "min_rating = 1\n",
    "max_rating = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_prep import load_data_surprise\n",
    "\n",
    "train_data = load_data_surprise(path_train, min_rating, max_rating)\n",
    "test_data= load_data_surprise(path_test, min_rating, max_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Surprise training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_prep import build_trainset_surprise\n",
    "\n",
    "trainset = build_trainset_surprise(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Surprise test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_prep import build_testset_surprise\n",
    "\n",
    "testset = build_testset_surprise(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896472"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of test examples\n",
    "\n",
    "len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the shortlisted models and evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normal Predictor (random predictions) as a baseline model\n",
    "\n",
    "Fit the algorithm and compute training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.0499\n",
      "RMSE: 1.3230\n"
     ]
    }
   ],
   "source": [
    "from modules.modeling import fit_surprise_algorithm\n",
    "\n",
    "normal_predictor = fit_surprise_algorithm(NormalPredictor(), trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.0505\n",
      "RMSE: 1.3238\n"
     ]
    }
   ],
   "source": [
    "from modules.modeling import evaluate_performance\n",
    "\n",
    "np_predictions_test = evaluate_performance(normal_predictor, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/normal_predictor\n"
     ]
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "dump.dump('../models/normal_predictor', algo=normal_predictor, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/normal_predictor_pred_test\n"
     ]
    }
   ],
   "source": [
    "# Save the algorithm along with test predictions\n",
    "\n",
    "dump.dump('../models/normal_predictor_pred_test', algo=normal_predictor, \n",
    "          predictions=np_predictions_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Slope One\n",
    "\n",
    "Fit the algorithm and compute training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.5847\n",
      "RMSE: 0.7582\n"
     ]
    }
   ],
   "source": [
    "slope_one = fit_surprise_algorithm(SlopeOne(), trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.6583\n",
      "RMSE: 0.8460\n"
     ]
    }
   ],
   "source": [
    "so_predictions_test = evaluate_performance(slope_one, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/slope_one\n"
     ]
    }
   ],
   "source": [
    "dump.dump('../models/slope_one', algo=slope_one, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/slope_one_pred_test\n"
     ]
    }
   ],
   "source": [
    "# Save with test predictions\n",
    "\n",
    "dump.dump('../models/slope_one_pred_test', algo=slope_one, \n",
    "          predictions=so_predictions_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SVD++\n",
    "\n",
    "Hyperparameters: \n",
    "- n_factors=8\n",
    "- n_epochs=15\n",
    "- lr_all=0.01\n",
    "- reg_all=0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.5853\n",
      "RMSE: 0.7573\n"
     ]
    }
   ],
   "source": [
    "svdpp = fit_surprise_algorithm(SVDpp(n_factors=8, n_epochs=15, lr_all=0.01, reg_all=0.02), \n",
    "                               trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.6293\n",
      "RMSE: 0.8161\n"
     ]
    }
   ],
   "source": [
    "svdpp_predictions_test = evaluate_performance(svdpp, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/svdpp\n"
     ]
    }
   ],
   "source": [
    "dump.dump('../models/svdpp', algo=svdpp, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/svdpp_pred_test\n"
     ]
    }
   ],
   "source": [
    "# Save together with test predictions\n",
    "\n",
    "dump.dump('../models/svdpp_pred_test', algo=svdpp, \n",
    "          predictions=svdpp_predictions_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9198658742269696"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean rating for test data = 3.92\n",
    "\n",
    "mean_rating_test = np.mean([row[2] for row in testset])\n",
    "mean_rating_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 k-NN Baseline: item-based\n",
    "\n",
    "Fit the algorithm and compute training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "MAE:  0.3311\n",
      "RMSE: 0.4450\n"
     ]
    }
   ],
   "source": [
    "from modules.modeling import fit_surprise_algorithm\n",
    "\n",
    "# Similarity measure configuration\n",
    "sim_options = {'name': 'pearson_baseline',  # recommended in the surprise docs\n",
    "               'user_based': False,  # compute similarities between items\n",
    "               'min_support': 10}  # min number of common users for the similarity not to be zero\n",
    "\n",
    "# Fit the algorithm\n",
    "knn_baseline_items = fit_surprise_algorithm(KNNBaseline(sim_options=sim_options), \n",
    "                                            trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.6041\n",
      "RMSE: 0.7974\n"
     ]
    }
   ],
   "source": [
    "from modules.modeling import evaluate_performance\n",
    "\n",
    "knn_predictions_test = evaluate_performance(knn_baseline_items, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/knn_baseline_items\n"
     ]
    }
   ],
   "source": [
    "from surprise import dump\n",
    "\n",
    "dump.dump('../models/knn_baseline_items', algo=knn_baseline_items, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/knn_baseline_items_pred_test\n"
     ]
    }
   ],
   "source": [
    "# Save with test predictions\n",
    "\n",
    "dump.dump('../models/knn_baseline_items_pred_test', algo=knn_baseline_items, \n",
    "          predictions=knn_predictions_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "\n",
    "- There are quite close results on the test set for the Slope One (MAE=0.659), SVD++ (MAE=0.629) and k-NN Baseline item-based (MAE=0.604)\n",
    "- The k-NN Baseline model showed the best performance. The mean absolute error on the test set has been reduced by 42% compared to the Normal Predictor (MAE=1.050). \n",
    "- On average, rating predictions by k-NN Baseline are off by 15% (mean_rating=3.92 for test data).\n",
    "\n",
    "\n",
    "### Next steps: \n",
    "- Create an app with k-NN Baseline or SVD++ model for demo (to get book recommendations for a given user id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a698be0b48003a334c31a3f8e5600c0b8c4ac4b13c41f594d6ef3259800b67f3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('bootcamp_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
